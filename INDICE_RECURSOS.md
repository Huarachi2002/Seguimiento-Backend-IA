# üìö √çndice de Recursos - Integraci√≥n Hugging Face con GPU

## üéØ ¬øPor d√≥nde empezar?

### Si eres principiante:
1. **[INICIO_RAPIDO.md](INICIO_RAPIDO.md)** - Comienza aqu√≠ (10 pasos r√°pidos)
2. **[TUTORIAL_GPU_COMPLETO.md](TUTORIAL_GPU_COMPLETO.md)** - Tutorial detallado paso a paso

### Si ya sabes lo que haces:
1. **[verificar_gpu.py](#scripts-de-verificaci√≥n)** - Detecta GPU y CUDA
2. **[instalar_con_gpu.ps1](#scripts-de-instalaci√≥n)** - Instala todo autom√°ticamente
3. **[.env.example](#configuraci√≥n)** - Configura variables de entorno

---

## üìÅ Scripts de Verificaci√≥n

### `verificar_gpu.py`
**Prop√≥sito:** Detecta tu GPU, versi√≥n de CUDA, y recomienda instalaci√≥n de PyTorch

**Cu√°ndo usar:**
- Antes de instalar dependencias
- Para verificar que PyTorch detecte tu GPU
- Para diagnosticar problemas de CUDA

**Uso:**
```powershell
python verificar_gpu.py
```

**Salida:**
```
‚úÖ GPU NVIDIA detectada: NVIDIA GeForce RTX 3060
‚úÖ Versi√≥n de CUDA: 12.1
üì¶ Comando recomendado: pip install torch --index-url ...
```

---

### `verificar_sistema.py`
**Prop√≥sito:** Verificaci√≥n completa del sistema antes de iniciar

**Qu√© verifica:**
- ‚úÖ Versi√≥n de Python
- ‚úÖ GPU y CUDA
- ‚úÖ Dependencias instaladas
- ‚úÖ Redis funcionando
- ‚úÖ Archivo .env configurado
- ‚úÖ Estructura del proyecto

**Cu√°ndo usar:**
- Despu√©s de instalar todo
- Antes de iniciar la aplicaci√≥n
- Para diagnosticar problemas

**Uso:**
```powershell
python verificar_sistema.py
```

**Salida:**
```
‚úÖ Python: 3.11.5
‚úÖ CUDA disponible: S√≠
‚úÖ Dependencias: Todas instaladas
‚úÖ Redis: Conectado
‚úÖ Configuraci√≥n: .env correcto
‚úÖ SISTEMA LISTO PARA INICIAR
```

---

### `probar_modelo.py`
**Prop√≥sito:** Probar modelos de Hugging Face antes de integrarlos

**Qu√© hace:**
- Descarga y carga el modelo
- Muestra uso de VRAM
- Genera una respuesta de prueba
- Mide velocidad de inferencia

**Cu√°ndo usar:**
- Para elegir el modelo correcto
- Para verificar que cabe en tu GPU
- Para ver la calidad de respuestas

**Uso:**
```powershell
python probar_modelo.py
```

**Flujo interactivo:**
```
1. DialoGPT Small (350MB)
2. DialoGPT Medium (800MB) ‚≠ê
3. DialoGPT Large (1.5GB)
Selecciona: 2

‚úÖ Modelo cargado en cuda
üìä Memoria GPU usada: 1.62 GB
üí¨ Respuesta generada exitosamente
```

---

## üîß Scripts de Instalaci√≥n

### `instalar_con_gpu.ps1`
**Prop√≥sito:** Instalaci√≥n autom√°tica con detecci√≥n de CUDA

**Qu√© hace:**
1. Detecta versi√≥n de CUDA autom√°ticamente
2. Instala PyTorch con la versi√≥n correcta
3. Instala todas las dependencias
4. Verifica que todo funcione

**Cu√°ndo usar:**
- Primera instalaci√≥n del proyecto
- Despu√©s de cambiar de GPU
- Para reinstalar todo

**Uso:**
```powershell
.\instalar_con_gpu.ps1
```

**Proceso:**
```
Detectando CUDA... ‚úÖ CUDA 12.1
Instalando PyTorch... ‚úÖ
Instalando dependencias... ‚úÖ
Verificando instalaci√≥n... ‚úÖ
¬°Listo para usar!
```

---

## üìñ Documentaci√≥n

### `INICIO_RAPIDO.md`
**Para:** Principiantes que quieren arrancar r√°pido

**Contiene:**
- 9 pasos numerados
- Comandos copy-paste
- Soluciones r√°pidas a problemas comunes
- Enlaces a documentaci√≥n detallada

**Tiempo estimado:** 15-30 minutos

---

### `TUTORIAL_GPU_COMPLETO.md`
**Para:** Usuarios que quieren entender cada paso

**Contiene:**
- Explicaciones detalladas
- Por qu√© hacer cada paso
- Qu√© esperar en cada paso
- Troubleshooting exhaustivo
- Ejemplos de salida

**Secciones:**
1. Verificaci√≥n del Sistema
2. Instalaci√≥n de Dependencias
3. Prueba de Modelos
4. Configuraci√≥n del Proyecto
5. Actualizaci√≥n del Model Loader
6. Inicio de la Aplicaci√≥n
7. Soluci√≥n de Problemas

**Tiempo estimado:** 1-2 horas (primera vez)

---

### `MODELOS_RECOMENDADOS.md`
**Para:** Elegir el modelo correcto para tu caso de uso

**Contiene:**
- Tabla comparativa de modelos
- Requisitos de VRAM
- Pros y contras de cada modelo
- Recomendaciones por GPU
- Ejemplos de configuraci√≥n

**Modelos cubiertos:**
- DialoGPT (Small, Medium, Large)
- BlenderBot
- FLAN-T5
- GPT-2

**Recomendaci√≥n principal:** `microsoft/DialoGPT-medium`

---

### `GUIA_REDIS_COMPLETA.md`
**Para:** Entender c√≥mo funciona Redis en el proyecto

**Contiene:**
- Arquitectura de Redis
- Implementaci√≥n del cliente
- Repository pattern
- TTL y expiraci√≥n
- Rate limiting
- Endpoints de debugging

---

## ‚öôÔ∏è Configuraci√≥n

### `.env.example`
**Prop√≥sito:** Plantilla de configuraci√≥n

**Variables principales:**

**Modelo IA:**
```env
MODEL_NAME=microsoft/DialoGPT-medium
DEVICE=cuda
MAX_LENGTH=150
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50
```

**Redis:**
```env
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
CONVERSATION_TTL=3600
```

**Aplicaci√≥n:**
```env
APP_NAME="WhatsApp AI Assistant"
ENVIRONMENT=development
LOG_LEVEL=INFO
PORT=8000
```

**Uso:**
```powershell
Copy-Item .env.example .env
notepad .env
```

---

### `requirements.txt`
**Prop√≥sito:** Lista de dependencias Python

**Dependencias principales:**
- `torch==2.6.0` - PyTorch (instalar con CUDA primero)
- `transformers==4.47.1` - Hugging Face
- `fastapi==0.115.4` - Framework web
- `redis==5.2.1` - Cliente Redis
- `pydantic-settings==2.6.1` - Configuraci√≥n

**Uso:**
```powershell
pip install -r requirements.txt
```

**Nota:** Instala PyTorch con CUDA primero usando `instalar_con_gpu.ps1`

---

## üéì Flujo de Trabajo Recomendado

### Primera vez (Setup completo):

```
1. verificar_gpu.py          ‚Üí Verificar hardware
2. instalar_con_gpu.ps1      ‚Üí Instalar todo
3. Copy .env.example a .env  ‚Üí Configurar
4. probar_modelo.py          ‚Üí Elegir modelo
5. docker-compose up redis   ‚Üí Iniciar Redis
6. verificar_sistema.py      ‚Üí Verificar todo
7. uvicorn app.main:app      ‚Üí Iniciar app
```

**Tiempo total:** 30-60 minutos

---

### Cambiar de modelo:

```
1. probar_modelo.py          ‚Üí Probar nuevo modelo
2. Editar .env               ‚Üí MODEL_NAME=nuevo_modelo
3. Reiniciar aplicaci√≥n      ‚Üí Ctrl+C y volver a iniciar
```

**Tiempo:** 5-10 minutos

---

### Debugging:

```
1. verificar_sistema.py      ‚Üí Ver qu√© falla
2. Consultar TUTORIAL        ‚Üí Secci√≥n "Soluci√≥n de Problemas"
3. Verificar logs            ‚Üí Ver errores espec√≠ficos
4. Re-ejecutar instalaci√≥n   ‚Üí instalar_con_gpu.ps1 si es necesario
```

---

## üìä Comparaci√≥n de Modelos (Resumen)

| Modelo | Tama√±o | VRAM | Velocidad | Calidad | Recomendado para |
|--------|--------|------|-----------|---------|------------------|
| **DialoGPT-small** | 350MB | 2GB+ | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | GPUs limitadas, pruebas |
| **DialoGPT-medium** | 800MB | 4GB+ | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | **Producci√≥n general** ‚≠ê |
| **DialoGPT-large** | 1.5GB | 8GB+ | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | GPUs potentes, mejor calidad |
| BlenderBot | 1.6GB | 6GB+ | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Conversaciones naturales |

---

## üÜò Problemas Comunes

### GPU no detectada
```powershell
# Verificar drivers
nvidia-smi

# Reinstalar PyTorch
.\instalar_con_gpu.ps1
```

### Out of memory
```env
# En .env, cambiar a modelo m√°s peque√±o
MODEL_NAME=microsoft/DialoGPT-small
MAX_LENGTH=100
```

### Redis no conecta
```powershell
docker-compose up -d redis
```

### Imports no funcionan
```powershell
# Verificar entorno virtual activo
.\venv\Scripts\Activate.ps1

# Reinstalar dependencias
.\instalar_con_gpu.ps1
```

---

## üìû Recursos Externos

- **Hugging Face Hub:** https://huggingface.co/models
- **PyTorch CUDA:** https://pytorch.org/get-started/locally/
- **FastAPI Docs:** https://fastapi.tiangolo.com/
- **Redis Docs:** https://redis.io/docs/

---

## ‚úÖ Checklist de Producci√≥n

Antes de deploy:

- [ ] `verificar_sistema.py` pasa todas las verificaciones
- [ ] Modelo probado con datos reales
- [ ] Variables de entorno de producci√≥n configuradas
- [ ] Redis configurado con password
- [ ] PostgreSQL configurado (persistencia)
- [ ] Logs configurados
- [ ] Monitoreo de GPU activo
- [ ] Backup strategy definida
- [ ] Rate limiting configurado
- [ ] SSL/TLS habilitado

---

**√öltima actualizaci√≥n:** Enero 2024  
**Versi√≥n del proyecto:** 1.0.0  
**Mantenedor:** Tu equipo
