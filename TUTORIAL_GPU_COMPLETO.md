# üöÄ Tutorial Completo: Integrar Hugging Face con GPU en FastAPI

## üìã √çndice
1. [Verificaci√≥n del Sistema](#paso-1-verificaci√≥n-del-sistema)
2. [Instalaci√≥n de Dependencias](#paso-2-instalaci√≥n-de-dependencias)
3. [Prueba de Modelos](#paso-3-prueba-de-modelos)
4. [Configuraci√≥n del Proyecto](#paso-4-configuraci√≥n-del-proyecto)
5. [Actualizar el Model Loader](#paso-5-actualizar-el-model-loader)
6. [Probar la Aplicaci√≥n](#paso-6-probar-la-aplicaci√≥n)
7. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## PASO 1: Verificaci√≥n del Sistema

### ¬øQu√© vamos a hacer?
Verificar que tu GPU est√° correctamente configurada y detectar la versi√≥n de CUDA.

### Comandos a ejecutar:

```powershell
# Activar el entorno virtual
.\venv\Scripts\Activate.ps1

# Ejecutar el script de verificaci√≥n
python verificar_gpu.py
```

### ¬øQu√© deber√≠a ver?

**Si tienes GPU:**
```
============================================================
VERIFICACI√ìN DE GPU Y CUDA
============================================================

GPU NVIDIA detectada:
NVIDIA GeForce RTX 3060
Versi√≥n de CUDA: 12.1

RECOMENDACI√ìN DE INSTALACI√ìN:
Para instalar PyTorch con CUDA 12.1, ejecuta:
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

**Si NO tienes GPU:**
```
No se detect√≥ GPU NVIDIA o nvidia-smi no est√° disponible.
Se usar√° CPU para el modelo.
```

### ‚ö†Ô∏è Importante:
- **Anota la versi√≥n de CUDA** que detect√≥ (ejemplo: 12.1)
- Si no detecta GPU pero tienes una NVIDIA, necesitas instalar los drivers:
  - Descarga desde: https://www.nvidia.com/Download/index.aspx

---

## PASO 2: Instalaci√≥n de Dependencias

### Opci√≥n A: Instalaci√≥n Autom√°tica (Recomendado)

```powershell
# Ejecutar el script de instalaci√≥n autom√°tica
.\instalar_con_gpu.ps1
```

**Este script hace:**
1. ‚úÖ Detecta tu versi√≥n de CUDA autom√°ticamente
2. ‚úÖ Instala PyTorch con soporte GPU correcto
3. ‚úÖ Instala todas las dependencias del proyecto
4. ‚úÖ Verifica que todo funcione

**Tiempo estimado:** 5-10 minutos (dependiendo de tu internet)

### Opci√≥n B: Instalaci√≥n Manual

Si prefieres hacerlo paso a paso:

#### 1. Instalar PyTorch con GPU

**Para CUDA 12.x:**
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

**Para CUDA 11.x:**
```powershell
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

**Para CPU (sin GPU):**
```powershell
pip install torch torchvision torchaudio
```

#### 2. Instalar Transformers y otras dependencias

```powershell
pip install transformers==4.47.1
pip install accelerate==1.2.1
pip install redis==5.2.1
pip install fastapi==0.115.4
pip install uvicorn==0.34.0
pip install pydantic==2.10.3
pip install pydantic-settings==2.6.1
pip install python-dotenv==1.0.1
```

#### 3. Verificar instalaci√≥n

```powershell
python -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA disponible: {torch.cuda.is_available()}')"
```

**Deber√≠a mostrar:**
```
PyTorch: 2.6.0+cu124
CUDA disponible: True
```

---

## PASO 3: Prueba de Modelos

### ¬øQu√© vamos a hacer?
Probar diferentes modelos antes de elegir uno para tu aplicaci√≥n.

### Ejecutar el probador de modelos:

```powershell
python probar_modelo.py
```

### Flujo del probador:

```
ü§ñ PROBADOR DE MODELOS DE HUGGING FACE
============================================================

üîç VERIFICACI√ìN DE SISTEMA
‚úÖ CUDA disponible: S√≠
‚úÖ GPU detectada: NVIDIA GeForce RTX 3060
‚úÖ VRAM total: 12.00 GB
‚úÖ VRAM disponible: 11.50 GB

Modelos disponibles:

1. DialoGPT Small (350MB) - R√°pido
   microsoft/DialoGPT-small

2. DialoGPT Medium (800MB) - Recomendado
   microsoft/DialoGPT-medium

3. DialoGPT Large (1.5GB) - Mejor calidad
   microsoft/DialoGPT-large

4. BlenderBot (1.6GB) - Mejor conversacional
   facebook/blenderbot-400M-distill

5. FLAN-T5 Base (900MB) - Tareas espec√≠ficas
   google/flan-t5-base

0. Ingresar otro modelo manualmente

Selecciona un modelo (1-5 o 0):
```

### üí° Recomendaciones seg√∫n tu GPU:

| VRAM | Modelo Recomendado | Opci√≥n |
|------|-------------------|--------|
| 4GB | DialoGPT Small | Opci√≥n 1 |
| 6-8GB | **DialoGPT Medium** | **Opci√≥n 2** ‚≠ê |
| 12GB+ | DialoGPT Large o BlenderBot | Opci√≥n 3 o 4 |

### Ejemplo de ejecuci√≥n:

```
Selecciona un modelo (1-5 o 0): 2

¬øUsar prompt de prueba por defecto? (s/n): s

ü§ñ PROBANDO MODELO: microsoft/DialoGPT-medium
============================================================

1Ô∏è‚É£  Cargando tokenizer...
‚úÖ Tokenizer cargado

2Ô∏è‚É£  Cargando modelo...
   Esto puede tomar varios minutos la primera vez...
   El modelo se descargar√° a: ~/.cache/huggingface/

‚úÖ Modelo cargado en: cuda
üìä Memoria GPU usada por el modelo: 1.62 GB

3Ô∏è‚É£  Generando respuesta de prueba...

üí¨ Pregunta: Hola, necesito agendar una cita m√©dica

ü§ñ Respuesta: Claro, estar√© encantado de ayudarte. ¬øPara qu√© especialidad necesitas la cita?

============================================================
‚úÖ PRUEBA COMPLETADA EXITOSAMENTE
============================================================
```

### ‚ö†Ô∏è Notas importantes:

- **Primera ejecuci√≥n**: El modelo se descarga (puede tardar 5-10 min)
- **Ubicaci√≥n**: Se guarda en `C:\Users\PC\.cache\huggingface\`
- **Siguientes ejecuciones**: Ser√°n instant√°neas (ya est√° descargado)

---

## PASO 4: Configuraci√≥n del Proyecto

### 1. Editar el archivo `.env`

Abre tu archivo `.env` y agrega/modifica estas l√≠neas:

```env
# Configuraci√≥n del Modelo de IA
MODEL_NAME=microsoft/DialoGPT-medium
DEVICE=cuda
MAX_LENGTH=150
TEMPERATURE=0.7
TOP_P=0.9
TOP_K=50

# Redis (ya configurado previamente)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_DECODE_RESPONSES=true
CONVERSATION_TTL=3600

# PostgreSQL (para futuro)
DATABASE_URL=postgresql://user:password@localhost:5432/whatsapp_db
```

### 2. Valores seg√∫n tu GPU:

**Para GPU con 4GB VRAM (DialoGPT Small):**
```env
MODEL_NAME=microsoft/DialoGPT-small
DEVICE=cuda
MAX_LENGTH=100
```

**Para GPU con 6-8GB VRAM (DialoGPT Medium):**
```env
MODEL_NAME=microsoft/DialoGPT-medium
DEVICE=cuda
MAX_LENGTH=150
```

**Para GPU con 12GB+ VRAM (DialoGPT Large):**
```env
MODEL_NAME=microsoft/DialoGPT-large
DEVICE=cuda
MAX_LENGTH=200
```

**Para CPU (sin GPU):**
```env
MODEL_NAME=microsoft/DialoGPT-small
DEVICE=cpu
MAX_LENGTH=100
```

### 3. Verificar archivo `app/core/config.py`

Ya deber√≠a estar configurado con Pydantic Settings:

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Modelo IA
    model_name: str = "microsoft/DialoGPT-medium"
    device: str = "cuda"
    max_length: int = 150
    temperature: float = 0.7
    top_p: float = 0.9
    top_k: int = 50
    
    # Redis
    redis_host: str = "localhost"
    redis_port: int = 6379
    # ... resto de configuraci√≥n

    class Config:
        env_file = ".env"
        case_sensitive = False
```

‚úÖ **Los valores del `.env` tendr√°n prioridad sobre los valores por defecto**

---

## PASO 5: Actualizar el Model Loader

### Verificar archivo `app/infrastructure/ai/model_loader.py`

Aseg√∫rate de que est√© actualizado para usar GPU:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)

class ModelLoader:
    _instance = None
    _model = None
    _tokenizer = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def load_model(self):
        """Carga el modelo y tokenizer con soporte GPU."""
        if self._model is None or self._tokenizer is None:
            logger.info(f"Cargando modelo: {settings.model_name}")
            logger.info(f"Dispositivo: {settings.device}")
            
            # Cargar tokenizer
            self._tokenizer = AutoTokenizer.from_pretrained(settings.model_name)
            
            # Determinar dtype seg√∫n dispositivo
            dtype = torch.float16 if settings.device == "cuda" else torch.float32
            
            # Cargar modelo
            self._model = AutoModelForCausalLM.from_pretrained(
                settings.model_name,
                torch_dtype=dtype
            )
            
            # Mover a GPU si est√° disponible
            self._model = self._model.to(settings.device)
            
            logger.info(f"Modelo cargado exitosamente en {settings.device}")
            
            if settings.device == "cuda":
                memoria_usada = torch.cuda.memory_allocated(0) / 1e9
                logger.info(f"Memoria GPU usada: {memoria_usada:.2f} GB")
        
        return self._model, self._tokenizer
    
    @property
    def model(self):
        if self._model is None:
            self.load_model()
        return self._model
    
    @property
    def tokenizer(self):
        if self._tokenizer is None:
            self.load_model()
        return self._tokenizer

# Singleton global
model_loader = ModelLoader()
```

---

## PASO 6: Probar la Aplicaci√≥n

### 1. Iniciar Redis (si no est√° corriendo)

```powershell
docker-compose up -d redis
```

### 2. Iniciar la aplicaci√≥n FastAPI

```powershell
python -m uvicorn app.main:app --reload
```

### Qu√© deber√≠as ver en la consola:

```
INFO:     Started server process [12345]
INFO:     Waiting for application startup.
INFO:app.infrastructure.ai.model_loader:Cargando modelo: microsoft/DialoGPT-medium
INFO:app.infrastructure.ai.model_loader:Dispositivo: cuda
INFO:app.infrastructure.ai.model_loader:Modelo cargado exitosamente en cuda
INFO:app.infrastructure.ai.model_loader:Memoria GPU usada: 1.62 GB
INFO:app.infrastructure.redis.redis_client:Conectado a Redis en localhost:6379
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
```

‚úÖ **Si ves esto, todo est√° funcionando correctamente**

### 3. Probar el endpoint de chat

Abre otra terminal PowerShell y ejecuta:

```powershell
curl -X POST "http://localhost:8000/api/chat" `
  -H "Content-Type: application/json" `
  -d '{\"user_id\": \"test_user\", \"message\": \"Hola, necesito agendar una cita\"}'
```

**Respuesta esperada:**
```json
{
  "response": "Claro, estar√© encantado de ayudarte. ¬øPara qu√© especialidad necesitas la cita?",
  "user_id": "test_user",
  "timestamp": "2024-01-15T10:30:00"
}
```

### 4. Verificar uso de GPU en tiempo real

Abre una tercera terminal y ejecuta:

```powershell
nvidia-smi -l 1
```

Esto mostrar√° el uso de GPU cada segundo. Deber√≠as ver tu aplicaci√≥n Python usando VRAM.

---

## Soluci√≥n de Problemas

### ‚ùå Error: "CUDA out of memory"

**Problema:** Tu GPU no tiene suficiente VRAM para el modelo.

**Soluci√≥n:**
1. Usa un modelo m√°s peque√±o en `.env`:
   ```env
   MODEL_NAME=microsoft/DialoGPT-small
   MAX_LENGTH=100
   ```
2. O usa CPU:
   ```env
   DEVICE=cpu
   ```

### ‚ùå Error: "torch.cuda.is_available() returns False"

**Problema:** PyTorch no detecta la GPU.

**Soluciones:**
1. Verifica que instalaste PyTorch con CUDA:
   ```powershell
   pip uninstall torch
   pip install torch --index-url https://download.pytorch.org/whl/cu124
   ```
2. Verifica drivers NVIDIA:
   ```powershell
   nvidia-smi
   ```

### ‚ùå Error: "Model loading is slow"

**Problema:** Primera carga descarga el modelo (normal).

**Soluci√≥n:**
- Espera pacientemente (5-10 minutos la primera vez)
- Las siguientes veces ser√° instant√°neo
- Verifica tu conexi√≥n a internet

### ‚ùå Error: "Redis connection refused"

**Problema:** Redis no est√° corriendo.

**Soluci√≥n:**
```powershell
docker-compose up -d redis
```

### ‚ùå Error: Import errors en Python

**Problema:** Dependencias no instaladas.

**Soluci√≥n:**
```powershell
.\instalar_con_gpu.ps1
```

---

## üìä Checklist Final

Antes de poner en producci√≥n, verifica:

- [ ] GPU detectada correctamente (`nvidia-smi` funciona)
- [ ] PyTorch con CUDA instalado (`torch.cuda.is_available() = True`)
- [ ] Modelo probado con `probar_modelo.py`
- [ ] `.env` configurado con el modelo correcto
- [ ] Redis corriendo (`docker-compose ps`)
- [ ] FastAPI inicia sin errores
- [ ] Endpoint `/api/chat` responde correctamente
- [ ] Memoria GPU monitoreada y estable

---

## üéâ ¬°Felicidades!

Si llegaste hasta aqu√≠ y todos los pasos funcionaron, ahora tienes:

‚úÖ Un asistente de IA funcionando con GPU  
‚úÖ Respuestas m√°s r√°pidas (GPU vs CPU)  
‚úÖ Contexto conversacional con Redis  
‚úÖ API REST lista para WhatsApp  

### üìö Pr√≥ximos Pasos

1. **Integrar con WhatsApp**: Conectar con WhatsApp Business API
2. **Mejorar prompts**: Personalizar para tu caso de uso m√©dico
3. **Agregar PostgreSQL**: Guardar conversaciones permanentemente
4. **Monitoreo**: Configurar logs y m√©tricas
5. **Optimizaci√≥n**: Fine-tuning del modelo para tu dominio

---

## üìû Recursos Adicionales

- **Hugging Face Docs**: https://huggingface.co/docs
- **PyTorch CUDA Setup**: https://pytorch.org/get-started/locally/
- **FastAPI Docs**: https://fastapi.tiangolo.com/
- **Redis Docs**: https://redis.io/docs/

**Documentos del proyecto:**
- `MODELOS_RECOMENDADOS.md` - Comparativa de modelos
- `GUIA_REDIS_COMPLETA.md` - Redis en detalle
- `ARCHITECTURE.md` - Arquitectura del proyecto
